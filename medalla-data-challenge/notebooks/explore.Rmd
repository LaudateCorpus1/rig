---
title: "Exploring Medalla data"
author:
- name: Barnab√© Monnot
  url: https://twitter.com/barnabemonnot
  affiliation: Robust Incentives Group, Ethereum Foundation
  affiliation_url: https://github.com/ethereum/rig
date: "`r Sys.Date()`"
output:
  distill::distill_article:
    toc: yes
    toc_depth: 3
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
description: |
  Initial exploration.
---

```{r setup, include=FALSE}
library(tidyverse)
library(glue)
library(RPostgres)
library(DBI)
library(ineq)
library(PKI)
library(rmarkdown)
library(zoo)
library(data.table)

source(here::here("notebooks/lib.R"))
source(here::here("notebooks/pw.R"))

con <- dbConnect(RPostgres::Postgres(), user="chain", password=pw)

options(digits=10)
options(scipen = 999) 

# Make the plots a bit less pixellated
knitr::opts_chunk$set(dpi = 400)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)

# A minimal theme I like
newtheme <- theme_grey() + theme(
  axis.text = element_text(size = 9),
  axis.title = element_text(size = 12),
  axis.line = element_line(colour = "#000000"),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  panel.background = element_blank(),
  legend.title = element_text(size = 12),
  legend.text = element_text(size = 10),
  legend.box.background = element_blank(),
  legend.key = element_blank(),
  strip.text.x = element_text(size = 10),
  strip.background = element_rect(fill = "white")
)
theme_set(newtheme)

myred <- "#F05431"
myyellow <- "#FED152"
mygreen <- "#BFCE80"
```

```{r}
until_epoch <- 12125
slot_chunk_res <- 25000
epoch_resolution <- 12125

slots_per_epoch <- 32
until_slot <- until_epoch * slots_per_epoch
slots_per_year <- 365.25 * 24 * 60 * 60 / 12
epochs_per_year <- slots_per_year / slots_per_epoch
```

```{r}
all_bxs <- fread(here::here("rds_data/all_bxs.csv")) %>%
  .[slot <= until_slot,]
```

```{r}
all_ats <- fread(here::here("rds_data/all_ats.csv")) %>%
  .[slot <= until_slot,]
```

```{r}
all_vs <- fread(here::here("rds_data/all_vs.csv"))
```

```{r}
all_dps <- fread(here::here("rds_data/all_dps.csv")) %>%
  .[deposit_slot <= until_slot,]
```

```{r}
capture_balances <- function(df) {
  df %>%
    select(epoch = f_epoch, validator_index = f_validator_index,
           balance = f_balance, effective_balance = f_effective_balance) %>%
    mutate_all(as.numeric)
}

val_balances <- dbGetQuery(
  con, str_c("SELECT * FROM t_validator_balances WHERE f_epoch = ", until_epoch)) %>%
  capture_balances() %>%
  as.data.table()

initial_balances <- dbGetQuery(
  con, "SELECT * FROM t_validator_balances WHERE f_epoch = 0"
) %>%
  capture_balances() %>%
  as.data.table()

# val_balances <- dbFetch(res, n = 1000) %>% capture_balances()
# while(!dbHasCompleted(res)) {
#   val_balances <- val_balances %>%
#     bind_rows(dbFetch(res, n = 1000) %>% capture_balances())
# }
```

In this notebook we explore data from the [Medalla testnet](https://ethereum.org/en/eth2/get-involved/medalla-data-challenge/). We are looking at the `r (max(all_bxs$slot) + 1)` first slots.

<aside>
You can find [the source of this notebook](https://github.com/ethereum/rig/blob/master/medalla-data-challenge/notebooks/explore.Rmd) over at the RIG repository. The code to [generate the datasets](https://github.com/ethereum/rig/blob/master/medalla-data-challenge/notebooks/create_datasets.R) is also available.
</aside>

## Data sources

### Lighthouse block export

We use a fork of [Lakshman Sankar](https://twitter.com/lakshmansankar)'s [Lighthouse block exporter](https://github.com/barnabemonnot/lighthouse_block_export) to export attestations and blocks from the finalised chain until slot `r until_slot`.

We present the main datasets below:

#### `all_ats`

Each row in this dataset corresponds to an aggregate attestation included in a block.

```{r, layout="l-body-outset"}
all_ats %>%
  head() %>%
  paged_table()
```

#### `exploded_ats`

We cast the dataset above into a long format, such that each row corresponds to an individual attestation included in a block. Note that when this individual attestation is included multiple times over multiple aggregates, it appears multiple times in the dataset.

```{r, layout="l-body-outset"}
fread(here::here("rds_data/exploded_ats_0.csv")) %>%
  arrange(att_slot, committee_index, index_in_committee) %>%
  head() %>%
  paged_table()
```

#### `individual_ats`

`exploded_ats` is the "disaggregated" version of the aggregate attestations. To check for validator performance, we often don't need to check for every inclusion of their individual attestations. `individual_ats` contains these unique, individual attestations, tagged with some extra data such as their earliest inclusion and whether they attested correctly for the target checkpoint and the head.

```{r, layout="l-body-outset"}
fread(here::here("rds_data/individual_ats_0.csv")) %>%
  arrange(att_slot, committee_index, index_in_committee) %>%
  head() %>%
  paged_table()
```

#### `all_dps`

Validators are allowed to deposit more ETH into their eth2 accounts. Later on, we compute the reward obtained by a validator by comparing its initial balance with its current balance. We must then deduct deposits that were made since genesis.

```{r, layout="l-body-outset"}
all_dps %>%
  head() %>%
  paged_table()
```

<aside>
The validator index is not included in the `Deposit` object recorded on the beacon chain. We use data from the Weald dump, presented below, to recover the validator index from its public key.
</aside>

### Weald dump

[Jim McDonald](https://twitter.com/AttestantIO), from Attestant, kindly provided a treasure trove of data on the #medalla-data-challenge channel of the EthStaker Discord server. The two previous datasets could have legitimately been mined from Jim's data, but we like to get our hands dirty.

#### `all_cms`

Not too dirty though: obtaining the past record of committees (which validators are supposed to attend when) is much more computationally intensive, since it requires access to past states. Yet given that we have `r until_epoch` epochs in our dataset and a maximum of `r all_vs %>% nrow()` validators, a dataset compiling all committee assignments would have `r until_epoch * (all_vs %>% nrow())` rows, which is too much. When we need committee information, we'll pull it from the database and record intermediary datasets instead.

<aside>
Note that you can obtain past states from the [Lighthouse block exporter](https://github.com/barnabemonnot/lighthouse_block_export) too, but still need to compute the committees from them.
</aside>

```{r, layout="l-body-outset"}
dbGetQuery(
  con,
  str_c(
    "SELECT * FROM t_beacon_committees WHERE f_slot >= 0 AND f_slot < 1"
  )
) %>%
  capture_committee() %>%
  head() %>%
  paged_table()
```

#### `val_balances`

This dataset gives us validator state balances at the beginning of each epoch. Note that the _state balance_ (`balance`), the true ETH amount a validator deposited, is different from the effective balance (`effective_balance`), which measures the principal on which validators receive an interest.

<aside>
Rewards are not given out at the end of genesis epoch, so the balance would only change for rows where `epoch >= 2`.
</aside>

```{r, layout="l-body-outset"}
val_balances %>%
  head() %>%
  paged_table()
```

<!-- ### Which attestations vote for sources, targets or chain head outside of the known blocks? -->

<!-- ```{r} -->
<!-- correct_ats <- all_ats %>% -->
<!--   inner_join(all_bxs %>% select(block_root), by = c("source_block_root" = "block_root")) %>% # same number as `all_ats` -->
<!--   inner_join(all_bxs %>% select(block_root), by = c("target_block_root" = "block_root")) # smaller than `all_ats` -->
<!-- ``` -->

### Computed datasets

To ease the computational demands of this notebook, we record two datasets from which much of the analysis can be derived.

#### `stats_per_val`

For each validator, we compute a bunch of statistics, including:

- `included_ats`: The number of times their attestations were included
- `first_att`/`last_att`: The attesting slot of their earliest and latest attestation (used by [pintail](https://pintail.xyz/medalla-validator-taxonomy) to build validator types)
- `correct_targets`/`correct_heads`: How many times they correctly attested for the target checkpoint or the head
- `avg_delay`: Their average inclusion delay

```{r, layout="l-body-outset"}
fread(here::here("rds_data/stats_per_val.csv")) %>%
  select(validator_index, included_ats, first_att, last_att, correct_targets,
         correct_heads, avg_delay) %>%
  head() %>%
  paged_table()
```

#### `stats_per_slot`

We also record summary statistics for each slot. At `r until_slot` slots in our dataset, this remains manageable to query. We have the following fields:

- `included_ats`: How many attestations were received for the slot.
- `expected_ats`: How many attestations were expected for the slot.
- `correct_targets`/`correct_heads`: The number of correct target/head attestations for that slot.

```{r, layout="l-body-outset"}
fread(here::here("rds_data/stats_per_slot.csv")) %>%
  select(att_slot, included_ats, expected_ats, correct_targets, correct_heads) %>%
  head() %>%
  paged_table()
```


## Performance of duties

### Attester duties

We compare the number of included attestations with the number of expected attestations.

```{r}
fread(here::here("rds_data/stats_per_slot.csv")) %>%
  .[, slot_chunk:=att_slot %/% slot_chunk_res] %>%
  filter(slot_chunk != max(slot_chunk)) %>%
  group_by(slot_chunk) %>%
  summarise(percent_received = sum(included_ats) / sum(expected_ats) * 100) %>%
  ggplot() +
  geom_line(aes(x = slot_chunk * slot_chunk_res %/% slots_per_epoch, y = percent_received), colour = myred) +
  geom_point(aes(x = slot_chunk * slot_chunk_res %/% slots_per_epoch, y = percent_received),
             colour = myred) +
  geom_text(aes(
    x = slot_chunk * slot_chunk_res %/% slots_per_epoch, y = percent_received,
    label = round(percent_received, digits = 1)),
    colour = myred, alpha = 0.7, nudge_y = -4) +
  xlab("Epoch") +
  ylab("Percent attested and included") +
  ylim(0, 100)
```

Clearly something went very wrong circa epoch 2,500. This is now known as the [roughtime incident](https://medium.com/prysmatic-labs/eth2-medalla-testnet-incident-f7fbc3cc934a), an issue affecting the major validator client, Prysm. It took time for the network to recover, in the process demonstrating how the [quadratic inactivity leak mechanism](https://github.com/ethereum/rig/blob/master/eth2economics/code/beaconrunner2049/beacon_runner_2049.ipynb) works. Client diversity FTW!

### Proposer duties

How many blocks are there in the canonical chain?

```{r}
tibble(slot = 0:until_slot) %>%
  left_join(all_bxs %>%
              select(slot) %>%
              mutate(proposed = 1),
            by = c("slot" = "slot")) %>%
  replace_na(list(proposed = 0)) %>%
  mutate(slot_chunk = slot %/% slot_chunk_res) %>%
  filter(slot_chunk != max(slot_chunk)) %>%
  group_by(slot_chunk) %>%
  summarise(percent_proposed = sum(proposed) / n() * 100) %>%
  ggplot() +
  geom_line(aes(x = slot_chunk * slot_chunk_res %/% slots_per_epoch, y = percent_proposed), colour = myred) +
  geom_point(aes(x = slot_chunk * slot_chunk_res %/% slots_per_epoch, y = percent_proposed),
             colour = myred) +
  geom_text(aes(
    x = slot_chunk * slot_chunk_res %/% slots_per_epoch, y = percent_proposed,
    label = round(percent_proposed, digits = 1)),
    colour = myred, alpha = 0.7, nudge_y = -4) +
  xlab("Epoch") +
  ylab("Percent proposed and included") +
  ylim(0, 100)
```

Again, the same trough during the roughtime incident.

## Correctness of attestations

### Target checkpoint

Attestations vouch for some target checkpoint to justify. We can check whether they vouched for the correct one by comparing their `target_block_root` with the latest known block root as of the start of the attestation epoch (that's a mouthful). How many individual attestations correctly attest for the target?

```{r}
n_individual_ats <- fread(here::here("rds_data/stats_per_slot.csv")) %>%
  pull(included_ats) %>%
  sum()
n_correct_target_ats <- fread(here::here("rds_data/stats_per_slot.csv")) %>%
  pull(correct_targets) %>%
  sum()

tibble(
  Name = c("Individual attestations", "Correct target attestations", "Percent correct"),
  Value = c(n_individual_ats, n_correct_target_ats, round(n_correct_target_ats / n_individual_ats * 100, digits = 2)
  )
) %>%
  paged_table()
```

How does the correctness evolve over time?

```{r}
fread(here::here("rds_data/stats_per_slot.csv")) %>%
  .[, slot_chunk:=att_slot %/% slot_chunk_res] %>%
  .[, .(percent_correct_target=sum(correct_targets) / sum(included_ats) * 100), by=slot_chunk] %>%
  ggplot() +
  geom_line(aes(x = slot_chunk * slot_chunk_res %/% slots_per_epoch, y = percent_correct_target),
            colour = mygreen) +
  geom_point(aes(x = slot_chunk * slot_chunk_res %/% slots_per_epoch, y = percent_correct_target),
             colour = mygreen) +
  geom_text(aes(
    x = slot_chunk * slot_chunk_res %/% slots_per_epoch, y = percent_correct_target,
    label = round(percent_correct_target, digits = 1)),
    colour = mygreen, alpha = 0.7, nudge_y = -4) +
  xlab("Epoch") +
  ylab("Percent correct targets") +
  ylim(0, 100)
```

### Head of the chain

Attestations must also vote for the correct head of the chain, as returned by the [GHOST fork choice rule]. To check for correctness, one looks at the latest block known as of the attestation slot. Possibly, this block was proposed for the same slot as the attestation `att_slot`. When the `beacon_block_root` attribute of the attestation and the latest block root match, the head is correct!

```{r}
n_correct_head_ats <- fread(here::here("rds_data/stats_per_slot.csv")) %>%
  pull(correct_heads) %>%
  sum()

tibble(
  Name = c("Individual attestations", "Correct head attestations", "Percent correct"),
  Value = c(n_individual_ats, n_correct_head_ats, round(n_correct_head_ats / n_individual_ats * 100, digits = 2)
  )
) %>%
  paged_table()
```

How does the correctness evolve over time?

```{r}
fread(here::here("rds_data/stats_per_slot.csv")) %>%
  .[, slot_chunk:=att_slot %/% slot_chunk_res] %>%
  .[, .(percent_correct_head=sum(correct_heads) / sum(included_ats) * 100), by=slot_chunk] %>%
  ggplot() +
  geom_line(aes(x = slot_chunk * slot_chunk_res %/% slots_per_epoch, y = percent_correct_head),
            colour = "purple") +
  geom_point(aes(x = slot_chunk * slot_chunk_res %/% slots_per_epoch, y = percent_correct_head),
             colour = "purple") +
  geom_text(aes(
    x = slot_chunk * slot_chunk_res %/% slots_per_epoch, y = percent_correct_head,
    label = round(percent_correct_head, digits = 1)),
    colour = "purple", alpha = 0.7, nudge_y = -4) +
  xlab("Epoch") +
  ylab("Percent correct head") +
  ylim(0, 100)
```

<!-- ### Justification and finalisation -->

```{r}
# all_cms %>%
#   left_join(
#     correct_target_ats %>%
#       distinct() %>%
#       select(att_slot, committee_index, index_in_committee) %>%
#       mutate(voted = 1),
#     by = c("att_slot" = "att_slot", "committee_index" = "committee_index", "index_in_committee" = "index_in_committee")
#   ) %>%
#   replace_na(list(voted = 0)) %>%
#   inner_join(val_balances,
#              by = c("epoch" = "epoch", "validator_index" = "validator_index")) %>%
#   mutate(effective_balance = effective_balance / (1e9)) %>%
#   group_by(epoch) %>%
#   summarise(attested_stake = sum(voted * effective_balance),
#             percent_attested = sum(voted * balance) / sum(balance))
```

## Validator performance

Validators are rewarded for their performance, and penalised for failing to complete their tasks. We start with a crude measure of performance: the number of included attestations. It is a crude measure since (a) we do not discount the timeliness of the validator, measured by the inclusion delay and (b) we do not check that the attestation's attributes are correct (with the exception of the `source` attribute, since an incorrect source cannot possibly be included on-chain).

### Uptime-rewards curve I: Included attestations

We compare the percentage of included attestations with the (possibly negative) reward obtained by the validator.

```{r}
bxs_proposed_per_val <- all_vs %>%
  .[, .(validator_index)] %>%
  merge(
    all_bxs %>%
      .[, .(blocks_proposed=.N), by=proposer_index],
    by.x = c("validator_index"),
    by.y = c("proposer_index"),
    all.x = TRUE
  ) %>%
  setnafill(type = "const", fill = 0, cols = c("blocks_proposed"))
```

```{r}
included_ats_per_val <- fread(here::here("rds_data/stats_per_val.csv")) %>%
  .[, .(validator_index, included_ats)]
```

```{r}
# Doesn't work, we are missing deposits, e.g., from block 150495
#
# deposits_per_val <- all_dps %>%
#   merge(all_vs %>% select(validator_index, activation_epoch)) %>%
#   filter(activation_epoch == 0) %>%
#   group_by(validator_index) %>%
#   summarise(extra_balance = sum(amount)) %>%
#   union(
#     all_dps %>%
#       merge(all_vs %>% select(validator_index, activation_epoch)) %>%
#       filter(activation_epoch != 0) %>%
#       group_by(validator_index) %>%
#       summarise(extra_balance = sum(amount) - 32 * (10 ^ 9))
#   ) %>%
#   union(
#     initial_balances %>%
#       mutate(extra_balance = balance - 32 * (10 ^ 9)) %>%
#       select(validator_index, extra_balance) %>%
#       arrange(desc(extra_balance))
#   ) %>%
#   group_by(validator_index) %>%
#   summarise(extra_balance = sum(extra_balance))
# 
# rewards_per_val <- included_ats_per_val %>%
#   merge(bxs_proposed_per_val, by="validator_index") %>%
#   merge(all_vs %>% select(validator_index, time_active)) %>%
#   merge(
#     deposits_per_val,
#     by="validator_index",
#     all.x = TRUE
#   ) %>%
#   setnafill(type="const", fill=0, cols=c("extra_balance")) %>%
#   merge(
#     val_balances,
#     by = c("validator_index" = "validator_index")
#   ) %>%
#   mutate(
#     true_balance = balance - extra_balance,
#     balance_diff = (true_balance - 32 * (10 ^ 9)) / balance * 100 * epochs_per_year / time_active
#   )
```

```{r}
rewards_per_val <- included_ats_per_val %>%
  merge(bxs_proposed_per_val, by="validator_index") %>%
  merge(all_vs %>% select(validator_index, time_active)) %>%
  merge(
    val_balances,
    by = c("validator_index" = "validator_index")
  ) %>%
  mutate(
    balance = if_else(balance < 16e9, 16e9+1, balance),
    round_balance = round(balance / (32e9)) * 32e9,
    true_rewards = balance - round_balance,
    balance_diff = true_rewards / (32e9) * 100 * epochs_per_year / time_active
  )
```


```{r}
rewards_per_val %>%
  filter(abs(balance_diff - mean(rewards_per_val$balance_diff)) <
           sd(rewards_per_val$balance_diff)) %>%
  mutate(percent_attested = included_ats / (time_active + 1) * 100) %>%
  ggplot() +
  geom_point(aes(x = percent_attested, y = balance_diff), alpha = 0.2, colour = myred) +
  geom_hline(yintercept = 0, colour = "steelblue", linetype = "dashed") +
  xlab("Percent of epochs attested") +
  ylab("Annualised reward (%)")
```

Who are the validators getting a negative return? We plot the same, showing how long a validator has been in service.

```{r}
uptime_rewards <- rewards_per_val %>%
  filter(abs(balance_diff - mean(rewards_per_val$balance_diff)) <
           sd(rewards_per_val$balance_diff)) %>%
  mutate(percent_attested = included_ats / (time_active + 1) * 100) %>%
  left_join(
    fread(here::here("rds_data/stats_per_val.csv")) %>%
      .[, percent_correct_heads:=correct_heads/included_ats]
  ) %>%
  replace_na(list(percent_correct_heads = 0))

uptime_rewards %>%
  ggplot() +
  geom_point(aes(x = percent_attested, y = balance_diff, colour = time_active), alpha = 0.05) +
  geom_hline(yintercept = 0, colour = "steelblue", linetype = "dashed") +
  scale_color_viridis_c() +
  # scale_color_manual(name = "Head correct", values = c(myred, mygreen)) +
  # facet_wrap(vars(blocks_proposed), ncol = 2) +
  xlab("Percent of epochs attested") +
  ylab("Annualised reward (%)")
```
<aside>
We only show return rates within one standard deviations of the mean. Very low reward rates are incurred by slashed validators.
</aside>

Recently activated validators have a much more balanced uptime-reward curve, with the higher performers getting positive returns. Meanwhile, validators who were active since the beginning tend to have smaller returns. This can be due to validator fatigue (validating for a while, then turning off the rig), but a fair number of early validators have high attestation performance yet low return. The roughtime incident is likely to blame here. Let's focus on these early validators.

```{r}
uptime_rewards %>%
  mutate(`Activation epoch` = if_else(time_active > 9000, "Early validators (activation before epoch 3000)", "Late validators (activation after epoch 3000)")) %>%
  ggplot() +
  geom_point(aes(x = percent_attested, y = balance_diff,
                 group=`Activation epoch`, color=`Activation epoch`),
             alpha = 0.2) +
  geom_hline(yintercept = 0, colour = "steelblue", linetype = "dashed") +
  scale_color_manual(values = c(myred, mygreen)) +
  facet_wrap(vars(`Activation epoch`)) +
  xlab("Percent of epochs attested") +
  ylab("Annualised reward (%)") +
  guides(color=FALSE)
```

Inactivity leaks push the uptime-rewards curve downwards. At best, validators can preserve their current balance if they validate optimally, with inclusion delay at 1 always. Most likely, active validators lose a small amount of their balance due to delay or attestation errors, while inactive validators leak much more.

### Uptime-rewards curve II: Inclusion delay

We turn our attention to the inclusion delay. Validators are rewarded for attesting timely, with higher rewards the earlier they are included in a block. We explode aggregates contained in the blocks to trace the earliest included attestation of each validator in an epoch.

```{r}
readRDS(here::here("rds_data/inclusion_delay_hist.rds")) %>%
  ggplot() +
  geom_col(aes(x = inclusion_delay, y = count), fill="steelblue") +
  scale_y_log10() +
  xlab("Inclusion delay") +
  ylab("Count (log10)")
```

Note that the y axis is given on a logarithmic scale. A high number of attestations have a low inclusion delay, which is good! Since attestations cannot be included more than 32 slots from their attesting slot, the distribution above is naturally capped at 32.

How is the inclusion delay correlated with the rewards? We look at validators with at least 70% of included attestations and activated after the roughtime incident to find out. 

```{r}
fread(here::here("rds_data/stats_per_val.csv")) %>%
  merge(rewards_per_val %>% as.data.table(),
        by=c("validator_index", "included_ats")) %>%
  .[time_active < 7000,] %>%
  .[included_ats > 0.7 * max(included_ats) & balance_diff < 100,] %>%
  ggplot() +
  geom_point(aes(x = avg_delay, y = balance_diff), alpha = 0.2, colour = myred) +
  geom_hline(yintercept = 0, colour = "steelblue", linetype = "dashed") +
  xlab("Average inclusion delay") +
  ylab("Annualised reward (%)")
```

The plot looks rather homogeneous...

## Aggregate attestations

eth2 is built to scale to tens of thousands of validators. This introduces overhead from message passing (and inclusion) when these validators are asked to vote on the canonical chain. To alleviate the beacon chain, votes (a.k.a. **attestations**) can be **aggregated**.

In particular, an attestation contains four attributes:

- The slot it is attesting for.
- Its vote for the head of the beacon chain, given by the fork choice rule.
- Its vote for the source, i.e., the last justified checkpoint in its view.
- Its vote for the target, i.e., the checkpoint to be justified in its view.

Since we expect validators to broadly agree in times of low latency, we also expect a lot of redundant attestations. We can aggregate such a set of attestations $I$ into a single, aggregate, attestation.

For each slot $s$, a committee of validator $C(s)$ is determined who is expected to attest for $s$. Assume that two aggregate attestations were formed from validators attesting for $s$, one aggregate of validators in set $I \subseteq C(s)$ and the other with validators in set $J \subseteq C(s)$. We have two cases:

- When the intersection of $I$ and $J$ is non-empty, we cannot aggregate the two aggregates further.
- When the intersection of $I$ and $J$ is empty, the two aggregates can themselves be aggregated, into one containing attestations from validator set $I \cup J$.

In the following, we look at redundant, clashing and individual attestations.

### How many individual attestations are contained in aggregates?

```{r message=FALSE}
all_ats %>%
  .[, .(count=.N), by=contained_ats] %>%
  ggplot() +
  geom_col(aes(x = contained_ats, y = count), fill=myred) +
  xlab("Number of attestations in aggregate") +
  ylab("Count")
```

A fairly high number of aggregate attestations included in a block are actually individual attestations. Nonetheless, a significant number of aggregates tally up between 50 and 100 attestations.

We can plot the same, weighing by the size of the validator set in the aggregate, to count how many individual attestations each size of aggregates included.

```{r}
all_ats %>%
  .[, .(count=.N * contained_ats), by=contained_ats] %>%
  ggplot() +
  geom_col(aes(x = contained_ats, y = count), fill=myred) +
  xlab("Number of attestations in aggregate") +
  ylab("Number of individual attestations")
```

Overall, we can plot the [Lorenz curve](https://en.wikipedia.org/wiki/Lorenz_curve) of aggregate attestations. This allows us to find out the share of attestations held by the 20% largest aggregates.

```{r}
L <- Lc(all_ats$contained_ats)
```

```{r}
L_tibble <- tibble(p = L$p, L = L$L) %>%
  filter(row_number() %% 100000 == 1 | row_number() == max(row_number()))

L_80q <- quantile(L$L, 0.8, names=FALSE) %>%
  round(digits = 2)

L_tibble %>%
  ggplot() +
  geom_line(aes(x = p, y = L), colour = myred, size = 1.1) +
  geom_abline(slope = 1, intercept = 0, linetype="dotted") +
  geom_vline(xintercept = 0.8, colour = "steelblue", linetype = "dotted", size = 1.1) +
  geom_hline(yintercept = L_80q, colour = "steelblue", size = 1.1) +
  scale_x_continuous(
    breaks = sort(c(c(0.8), with(L_tibble, pretty(range(p))))),
  ) +
  scale_y_continuous(
    breaks = sort(c(c(L_80q), with(L_tibble, pretty(range(p))))),
  ) +
  xlab("Aggregation percentile") +
  ylab("Cumulative share of attestations")
```


The answer is `r (100 - L_80q * 100)`%.

#### How much savings did aggregates provide?

We compare how many individual attestations exist to how many aggregates were included in blocks.

```{r}
n_individual_ats <- readRDS(here::here("rds_data/n_individual_ats.rds"))
n_aggregates <- all_ats %>% nrow()
tibble(Name = c("Individual attestations", "Included aggregates", "Savings ratio"),
       Value = c(n_individual_ats, n_aggregates,
                 round(n_individual_ats / n_aggregates, digits=2))) %>%
  paged_table()
```

We have `r round(n_individual_ats / n_aggregates, digits = 2)` times more individual attestations than aggregates, meaning that if we were not aggregating, we would have `r round(n_individual_ats / n_aggregates, digits = 2)` as much data on-chain.

### In how many aggregate attestations is a single attestation included?

We look at all _individual_ attestations in our dataset, i.e., individual, unaggregated votes, and measure how many times they were included in an aggregate.

```{r}
readRDS(here::here("rds_data/appearances_in_aggs.rds")) %>%
  ggplot() +
  geom_col(aes(x = appearances, y = count), fill=myred) +
  scale_y_log10() +
  xlab("Number of times included in an aggregate") +
  ylab("Count (log10)")
```

Most attestations were included in an aggregate once only

### How many redundant aggregate attestations are there?

We call **redundant** identical aggregate attestations (same four attributes and same set of validator indices) which are included in more than one block. It can happen when a block producer does not see that an aggregate was previously included (e.g., because of latency), or simply when the block producer doesn't pay attention and greedily adds as many aggregates as they know about.

```{r}
readRDS(here::here("rds_data/redundant_ats.rds")) %>%
  ggplot() +
  geom_col(aes(x = appearances, y = count), fill=myred) +
  xlab("Number of times redundant") +
  ylab("Count (log10)") +
  scale_y_log10()
```

The mode is 1, which is also the optimal case. A redundant aggregate does not have much purpose apart from bloating the chain.

### How many times did a block include the exact same aggregate attestation more than once?

We could call these **strongly redundant**, as this is pure waste.

```{r}
strong_redundant <- readRDS(here::here("rds_data/appearances_in_same_block.rds"))
n_strong_redundant_twice <- strong_redundant %>%
  pull(count) %>%
  pluck(2)
n_strong_redundant_over_twice <- strong_redundant %>%
  pull(count) %>%
  sum() - n_strong_redundant_twice - strong_redundant %>% pull(count) %>% pluck(1)
strong_redundant %>%
  paged_table()
```

We see that `r n_strong_redundant_twice` times, identical aggregates were included twice in a block.

### How many times were clashing attestations included in blocks?

We define **clashing** attestations as two aggregate attestations included in the same block, with identical attributes (same attesting slot, beacon chain head, source block and target block). We can further define the following two notions, assuming the two aggregate attestations include attestations of validator sets $I$ and $J$ respectively:

- **Weakly clashing:** the two aggregates have different validator indices, $I \neq J$.
- **Strongly clashing:** $I \neq J$ **and** $I \cap J \neq \emptyset$. The two aggregate attestations were incompatible, so could not be aggregated further.

We obtain how many times an attestation weakly clashes with itself, i.e., is included multiple times in a single block with different validator sets. For instance, if an aggregate attestation is included in the same block three times with a different set of validator indices each time, we record that this aggregate is weakly clashing three times with itself. We give below the histogram of this measure.

```{r}
readRDS(here::here("rds_data/weakly_clashing.rds")) %>%
  mutate(clashing = if_else(appearances == 1, "Non-weakly clashing", "Weakly clashing")) %>%
  ggplot() +
  geom_col(aes(x = appearances, y = count, group=clashing, fill=clashing)) +
  scale_y_log10() +
  scale_fill_manual("Clashing type", values = c(myyellow, myred)) +
  xlab("Times clashing") +
  ylab("Count (log10)") +
  xlim(0, 50)
```

From the plot above, we observe that some aggregates were included over 40 times in the same block, all with different sets of validator indices. Still, most aggregates were included once or a few times.

Finding weakly clashing attestations that are not strongly clashing (i.e., which could have been aggregated further) is left for future work as it is more computationally intensive. In particular, for a set of aggregates identical up to their validator indices, one must find which have an empty overlap.

Note that optimally aggregating a set of aggregates is NP-complete! Here is a reduction of the optimal aggregation problem to the [graph colouring](https://en.wikipedia.org/wiki/Graph_coloring). Set aggregate attestations as vertices in a graph, with an edge drawn between two vertices if the validator sets of the two aggregates have a non-empty overlap. In the graph colouring, we look for the minimum number of colours necessary to assign a colour to each vertex such that two connected vertices do not have the same colour. All vertices who share the same colour have an empty overlap, and thus can be combined into an aggregate. The minimum number of colours necessary to colour the graph tells us how few aggregates were necessary to combine a given set of aggregates further.

```{r}
# Disconnect from the database
dbDisconnect(con)
```

